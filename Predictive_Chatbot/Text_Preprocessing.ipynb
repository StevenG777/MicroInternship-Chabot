{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb9981b1",
   "metadata": {},
   "source": [
    "# Assignment 5: Text Preprocessing & Train/Test Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b1b8fa",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53d1a108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "import pickle\n",
    "from Assignment5_Intent2 import Intents\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a796b873",
   "metadata": {},
   "source": [
    "## Text Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "401c9c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_deletion_words(words, p):\n",
    "\n",
    "    #obviously, if there's only one word, don't delete it\n",
    "    if (len(words) == 1) or (len(words) == 0):\n",
    "        return words\n",
    "    \n",
    "    #randomly delete words with probability p\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        r = random.uniform(0, 1)\n",
    "        if r > p:\n",
    "            new_words.append(word)\n",
    "\n",
    "    #if you end up deleting all words, just return a random word\n",
    "    if len(new_words) == 0:\n",
    "        rand_int = random.randint(0, len(words)-1)\n",
    "        return [words[rand_int]]\n",
    "\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e443bef8",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ff02ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize lists\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "\n",
    "# Count Pattern Distribution\n",
    "intent_with_pattern_cnt = [len(IntentInfo[\"patterns\"]) for IntentInfo in Intents]\n",
    "max_cnt = max(intent_with_pattern_cnt)\n",
    "\n",
    "for IntentInfo in Intents:\n",
    "    # Initialize lists\n",
    "    extracted_documents = []\n",
    "    aug_documents = []\n",
    "    for pattern in IntentInfo['patterns']:\n",
    "        ## Text Preprocessing\n",
    "        # Lowercasing & White-Space Removal\n",
    "        pattern = pattern.lower().strip()\n",
    "        # Punctuation Removal\n",
    "        alphanum_pattern = ''.join([char if char.isalnum() else ' ' for char in pattern])\n",
    "        # Tokenization\n",
    "        tokenized_pattern = word_tokenize(alphanum_pattern)\n",
    "        # Stopword Removal\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        filtered_pattern = [word for word in tokenized_pattern if word not in stop_words]\n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_pattern = [lemmatizer.lemmatize(word) for word in filtered_pattern]      \n",
    "        \n",
    "        # Adding words, documents & classes\n",
    "        words.extend(lemmatized_pattern)\n",
    "        documents.append((lemmatized_pattern, IntentInfo['intent']))\n",
    "        if IntentInfo['intent'] not in classes:\n",
    "            classes.append(IntentInfo['intent'])\n",
    "            \n",
    "        # Store current pre-process text for text augmentation\n",
    "        extracted_documents.append(lemmatized_pattern)\n",
    "        \n",
    "    ## Text Augmentation\n",
    "    # Calculate numbers of augmented data samples to solve uneven pattern distribution\n",
    "    data_aug_cnt = max_cnt - len(IntentInfo[\"patterns\"])\n",
    "    intent = IntentInfo[\"intent\"]\n",
    "    # Perform augmentation through WORD RANDOM DELETION\n",
    "    if data_aug_cnt > 0:\n",
    "        for _ in range(data_aug_cnt):\n",
    "            deleted_pattern = random.choice(extracted_documents)\n",
    "            new_pattern = random_deletion_words(deleted_pattern, 0.5)\n",
    "            aug_documents.append((new_pattern, IntentInfo['intent']))\n",
    "    # Add to documents\n",
    "    documents.extend(aug_documents)\n",
    "\n",
    "## Text Preprocessing\n",
    "# Duplicate Removal\n",
    "words = list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2441a0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[468, 1492]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_with_pattern_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d57eab",
   "metadata": {},
   "source": [
    "## Generate Training & Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1636dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all feature & outcome data\n",
    "X_data = np.empty((len(documents), len(words)))\n",
    "Y_data = np.empty((len(documents), len(classes)))\n",
    "\n",
    "for i,doc in enumerate(documents):\n",
    "    # Initialize feature data --> Bag of Words (Text Embedding)\n",
    "    bag = []\n",
    "    # Extract pattern tokens\n",
    "    pattern = doc[0]\n",
    "    # Generate BoW\n",
    "    for word in words:\n",
    "        if word in pattern:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "    \n",
    "    # Initialize & Generate outcome data --> One Hot Encoding\n",
    "    output_row = list([0] * len(classes))\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "     \n",
    "    # Gather all data in BoW & OHE\n",
    "    X_data[i] = bag \n",
    "    Y_data[i] = output_row\n",
    "\n",
    "# Shuffle training data --> reduce bias\n",
    "random.shuffle(X_data)\n",
    "random.shuffle(Y_data)\n",
    "\n",
    "# Train/Test data split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c760fd4",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9d58ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'Y_train': Y_train,\n",
    "    'Y_test': Y_test\n",
    "}\n",
    "\n",
    "with open('train_test_data.pickle', 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "    \n",
    "with open('words.pickle', 'wb') as f:\n",
    "    pickle.dump(words, f)\n",
    "\n",
    "with open('classes.pickle', 'wb') as f:\n",
    "    pickle.dump(classes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f58cae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
